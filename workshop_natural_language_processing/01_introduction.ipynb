{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is nlp?\n",
    "\n",
    "NLP can be defined as a field of computer science that is concerned with enabling computer algorithms to understand, analyze, and generate natural languages.\n",
    "\n",
    "NLP works at different levels, which means that machines process and understand natural language at different levels. These levels are as follows:\n",
    "* Morphological level: This level deals with understanding word structure and word information.\n",
    "* Lexical level: This level deals with understanding the part of speech of the word.\n",
    "* Syntactic level: This level deals with understanding the syntactic analysis of a sentence, or parsing a sentence.\n",
    "* Semantic level: This level deals with understanding the actual meaning of a sentence.\n",
    "* Discourse level: This level deals with understanding the meaning of a sentence beyond just the sentence level, that is, considering the context.\n",
    "* Pragmatic level: This level deals with using real-world knowledge to understand the sentence.\n",
    "\n",
    "**Text analytics vs. NLP**\n",
    "Text analytics is the method of extracting meaningful insights and answering questions from text data, such as those to do with the length of sentences, length of words, word count, and finding words from the text. \n",
    "\n",
    "NLP, on the other hand, helps us in understanding the semantics and the underlying meaning of text, such as the sentiment of a sentence, top keywords in text, and parts of speech for different words. It is not just restricted to text data; voice (speech) recognition and analysis also come under the domain of NLP. \n",
    "\n",
    "Natural Language Understanding (NLU) and Natural Language Generation (NLG). A proper explanation of these terms is provided here:\n",
    "NLU: NLU refers to a process by which an inanimate object with computing power is able to comprehend spoken language. As mentioned earlier, Siri and Alexa use techniques such as Speech to Text to answer different questions, including inquiries about the weather, the latest news updates, live match scores, and more.\n",
    "NLG: NLG refers to a process by which an inanimate object with computing power is able to communicate with humans in a language that they can understand or is able to generate human-understandable text from a dataset. Continuing with the example of Siri or Alexa, ask one of them about the chances of rainfall in your city. It will reply with something along the lines\n",
    "\n",
    "NLP can be broadly categorized into two types:\n",
    "* Natural Language Understanding: NLU refers to a process by which machine is able to comprehend spoken language.\n",
    "* Natural Language Generation: NLG refers to a process by which a machine is able to communicate with humans in a language that they can understand or is able to generate human-understandable text from a dataset.\n",
    "\n",
    "# basic text analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether the word 'quick' belongs to that text \n",
    "\n",
    "def find_word(word, sentence):\n",
    "    return word in sentence\n",
    "find_word('quick', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out the index value of the word 'fox'\n",
    "\n",
    "def get_index(word, text):\n",
    "    return text.index(word)\n",
    "get_index('fox', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out the rank of the word 'lazy'\n",
    "\n",
    "get_index('lazy', sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brown'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the third word of the given text\n",
    "\n",
    "def get_word(text, rank):\n",
    "    return text.split()[rank]\n",
    "get_word(sentence, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nworb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the third word of the given sentence in reverse order\n",
    "\n",
    "get_word(sentence,2)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thedog'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the first and last words of the given sentence\n",
    "\n",
    "def concat_words(text):\n",
    "    \"\"\"\n",
    "    This method will concat first and last \n",
    "    words of given text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    first_word = words[0]\n",
    "    last_word = words[len(words)-1]\n",
    "    return first_word + last_word\n",
    "\n",
    "concat_words(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'brown', 'jumps', 'the', 'dog']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print words at even positions\n",
    "\n",
    "def get_even_position_words(text):\n",
    "    words = text.split()\n",
    "    return [words[i] for i in range(len(words)) if i%2 == 0]\n",
    " \n",
    "get_even_position_words(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the last three letters of the text\n",
    "\n",
    "def get_last_n_letters(text, n):\n",
    "    return text[-n:]\n",
    "get_last_n_letters(sentence,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'god yzal eht revo spmuj xof nworb kciuq ehT'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the text in reverse order\n",
    "\n",
    "def get_reverse(text):\n",
    "    return text[::-1]\n",
    "get_reverse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ehT kciuq nworb xof spmuj revo eht yzal god'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print each word in reverse order, maintaining sequence\n",
    "\n",
    "def get_word_reverse(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([word[::-1] for word in words])\n",
    "\n",
    "get_word_reverse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog lazy the over jumps fox brown quick The'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse the sequence of words in string\n",
    "\n",
    "wordList = sentence.split()\n",
    "' '.join(word for word in wordList[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# various steps in nlp\n",
    "\n",
    "## tokenization\n",
    "\n",
    "Tokenization refers to the procedure of splitting a sentence into its constituent parts â€” the words and punctuation that it is made up of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, download\n",
    "download(['punkt','averaged_perceptron_tagger','stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **download** method downloads the given data from NLTK. NLTK data contains different corpora and trained models. In the preceding example, we will be downloading the stop word list, 'punkt', and a perceptron tagger, which is used to implement parts of speech tagging using a structured algorithm. The data will be downloaded at nltk_data/corpora/ in the home directory of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'NLP', 'Fundamentals', '.']\n"
     ]
    }
   ],
   "source": [
    "print(get_tokens(\"I am reading NLP Fundamentals.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **word_tokenize** method is used to split the sentence into words/tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parts of speech (pos) tagging\n",
    "\n",
    "The **pos_tag** method returns a list of tuples in which every tuple consists of the word followed by the PoS tag.\n",
    "\n",
    "List of PoS tags: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('Fundamentals', 'NNS')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def get_tokens(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return words\n",
    "\n",
    "def get_pos(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "words  = get_tokens(\"I am reading NLP Fundamentals\")\n",
    "get_pos(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "# Check the list of stop words provided for English\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming languages\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from a sentence\n",
    "\n",
    "def remove_stop_words(sentence, stop_words):\n",
    "    return ' '.join([word for word in sentence if word not in stop_words])\n",
    "\n",
    "sentence = \"I am learning Python. \\\n",
    "    It is one of the most popular programming languages\"\n",
    "\n",
    "sentence_words = word_tokenize(sentence)\n",
    "\n",
    "print(remove_stop_words(sentence_words, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning Python . popular programming languages\n"
     ]
    }
   ],
   "source": [
    "# Add your own stop words to the stop word list\n",
    "\n",
    "stop_words.extend(['I', 'It', 'one'])\n",
    "print(remove_stop_words(sentence_words, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text normalization\n",
    "\n",
    "Text normalization is a process wherein different variations of text get converted into a standard form. We need to perform text normalization as there are some words that can mean the same thing as each other. There are various ways of normalizing text, such as spelling correction, stemming, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited the United States from the United Kingdom on 22-10-2018\n",
      "The United States and the United Kingdom are two superpowers\n"
     ]
    }
   ],
   "source": [
    "def normalize(text):\n",
    "    return text.replace(\"US\", \"United States\")   \\\n",
    "                .replace(\"UK\", \"United Kingdom\") \\\n",
    "                .replace(\"-18\", \"-2018\")\n",
    "\n",
    "sentence = \"I visited the US from the UK on 22-10-18\"\n",
    "normalized_sentence = normalize(sentence)\n",
    "print(normalized_sentence)\n",
    "\n",
    "normalized_sentence = normalize('The US and the UK are two superpowers')\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spelling correction\n",
    "\n",
    "**autocorrect** is a Python library used to correct the spelling of misspelled words for different languages. It provides a **spell** method which takes a word as input and returns the correct spelling of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "spell('Natureal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Procession deals with the art of extracting insights from Natural Languages\n"
     ]
    }
   ],
   "source": [
    "sentence = word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\")\n",
    "\n",
    "def correct_spelling(tokens):\n",
    "    sentence_corrected = ' '.join([spell(word) for word in tokens])\n",
    "    return sentence_corrected\n",
    "\n",
    "print(correct_spelling(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the wrongly spelled words have been corrected. But the word \"Processin\" was wrongly converted into \"Procession.\" This happened because to change \"Processin\" to \"Procession\" or \"Processing,\" an equal number of edits is required. To rectify this, we need to use other kinds of spelling correctors that are aware of context.\n",
    "\n",
    "## stemming\n",
    "\n",
    "Stemming is the process of converting words into their base forms.\n",
    "\n",
    "We will be using two algorithms provided by the NLTK library: \n",
    "* The porter stemmer is a rule-based algorithm that transforms words to their base form by removing suffixes from words. \n",
    "* The snowball stemmer is an improvement over the porter stemmer and is a little bit faster and uses less memory. \n",
    "\n",
    "In NLTK, this is done by the **stem** method provided by the **PorterStemmer** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product come fire battl battl\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "\n",
    "def get_stems(word, stemmer):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "porterStem = stem.PorterStemmer()\n",
    "\n",
    "production = get_stems(\"production\", porterStem)\n",
    "coming = get_stems(\"coming\", porterStem)\n",
    "firing = get_stems(\"firing\",porterStem)\n",
    "battling = get_stems(\"battling\",porterStem)\n",
    "\n",
    "# for snowball specify language\n",
    "stemmer = stem.SnowballStemmer(\"english\")\n",
    "battling_sb = get_stems(\"battling\", stemmer)\n",
    "\n",
    "print(production, coming, firing, battling, battling_sb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatization\n",
    "\n",
    "Lemmatization is the process of converting words to their base grammatical form rather than just randomly axing words (as stemming does?). An additional check is made by looking through a dictionary to extract the base form of a word. Getting more accurate results requires some additional information; for example, PoS tags along with words will help in getting better results.\n",
    "\n",
    "In the following, we use **WordNetLemmatizer**, which is an NLTK interface of WordNet (a freely available lexical English database that can be used to generate semantic relationships between words). It provides **lemmatize** method, which returns the lemma (grammatical base form) of a given word using WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product production coming battling\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_lemma(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "products = get_lemma('products')\n",
    "production = get_lemma('production')\n",
    "coming = get_lemma('coming')\n",
    "battling = get_lemma('battling')\n",
    "\n",
    "print(products, production, coming, battling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## name entity recognition (ner)\n",
    "\n",
    "NER is the process of extracting important entities, such as person names, place names, and organization names, from some given text. These are usually not present in dictionaries.\n",
    "\n",
    "Chunking is the process of grouping words together into chunks, which can be further used to find noun groups and verb groups, or can also be used for sentence partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "download('maxent_ne_chunker')\n",
    "download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Packt', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"We are reading a book published by Packt which is based out of Birmingham.\"\n",
    "\n",
    "def get_ner(text):\n",
    "    i = ne_chunk(pos_tag(word_tokenize(text)), binary=True)\n",
    "    return [a for a in i if len(a)==1]\n",
    "\n",
    "get_ner(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word sense disambiguation\n",
    "\n",
    "It is the process of mapping a word to the sense that it should carry. \n",
    "\n",
    "One of the algorithms to solve word sense disambiguation is the Lesk algorithm. It has a huge corpus in the background (generally WordNet is used) that contains definitions of all the possible synonyms of all the possible words in a language. Then it takes a word and the context as input and finds a match between the context and all the definitions of the word. The meaning with the highest number of matches with the context of the word will be returned.\n",
    "\n",
    "The **Lesk** module from NLTK takes a sentence and the word as input, and returns the meaning or definition of the word. The output of the **Lesk** method is **synset**, which contains the ID of the matched definition. These IDs can be matched with their definitions using the **definition** method of wsd.synset('word').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('savings_bank.n.02')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence1 = \"Keep your savings in the bank\"\n",
    "sentence2 = \"It's so risky to drive over the banks of the road\"\n",
    "\n",
    "def get_synset(sentence, word):\n",
    "    return lesk(word_tokenize(sentence), word)\n",
    "\n",
    "get_synset(sentence1,'bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('bank.v.07')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synset(sentence2,'bank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence boundary detection\n",
    "\n",
    "The **sent_tokenize** method is used to detect sentence boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are reading a book.',\n",
       " 'Do you know who is the publisher?',\n",
       " 'It is Packt.',\n",
       " 'Packt is based out of Birmingham.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "get_sentences(\"We are reading a book. Do you know who is the publisher? It is Packt. Packt is based out of Birmingham.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Donald John Trump is the current president of the USA.',\n",
       " 'Before joining politics, he was a businessman.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentences(\"Mr. Donald John Trump is the current president of the USA. Before joining politics, he was a businessman.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# activity: preprocessing of raw text\n",
    "\n",
    "1. Import the necessary libraries.\n",
    "2. Load the text corpus to a variable.\n",
    "3. Apply the tokenization process to the text corpus and print the first 20 tokens.\n",
    "4. Apply spelling correction on each token and print the initial 20 corrected tokens as well as the corrected text corpus.\n",
    "5. Apply PoS tags to each of the corrected tokens and print them.\n",
    "6. Remove stop words from the corrected token list and print the initial 20 tokens.\n",
    "7. Apply stemming and lemmatization to the corrected token list and then print the initial 20 tokens.\n",
    "8. Detect the sentence boundaries in the given text corpus and print the total number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import stem, pos_tag\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'reader', 'of', 'this', 'course', 'should', 'have', 'a', 'basic', 'knowledge', 'of', 'the', 'Python', 'programming', 'lenguage', '.', 'He/she', 'must', 'have', 'knowldge']\n"
     ]
    }
   ],
   "source": [
    "# 2, 3\n",
    "sentence = open(\"data/file.txt\", 'r').read()\n",
    "words = word_tokenize(sentence)\n",
    "print(words[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenguage has been corrected to: language\n",
      "knowldge has been corrected to: knowledge\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "def correct_sentence(words):\n",
    "    corrected_sentence = \"\"\n",
    "    corrected_word_list = []\n",
    "    for wd in words:\n",
    "        if wd not in string.punctuation:\n",
    "            wd_c = spell(wd)\n",
    "            if wd_c != wd:\n",
    "                print(wd+\" has been corrected to: \"+wd_c)\n",
    "                corrected_sentence = corrected_sentence+\" \"+wd_c\n",
    "                corrected_word_list.append(wd_c)\n",
    "            else:\n",
    "                corrected_sentence = corrected_sentence+\" \"+wd\n",
    "                corrected_word_list.append(wd)\n",
    "        else:\n",
    "            corrected_sentence = corrected_sentence + wd\n",
    "            corrected_word_list.append(wd)\n",
    "    return corrected_sentence, corrected_word_list\n",
    "\n",
    "corrected_sentence, corrected_word_list = correct_sentence(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The reader of this course should have a basic knowledge of the Python programming lenguage.\\nHe/she must have knowldge of data types in Python.He should be able to write functions,\\n and also have the ability to import and use libraries and packages in Python. Familiarity\\n with basic linguistics and probability is assumed although not required to fully\\n complete this course.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The reader of this course should have a basic knowledge of the Python programming language. He/she must have knowledge of data types in Python.He should be able to write functions, and also have the ability to import and use libraries and packages in Python. Familiarity with basic linguistics and probability is assumed although not required to fully complete this course.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'reader', 'of', 'this', 'course', 'should', 'have', 'a', 'basic', 'knowledge', 'of', 'the', 'Python', 'programming', 'language', '.', 'He/she', 'must', 'have', 'knowledge']\n"
     ]
    }
   ],
   "source": [
    "print(corrected_word_list[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('reader', 'NN'), ('of', 'IN'), ('this', 'DT'), ('course', 'NN'), ('should', 'MD'), ('have', 'VB'), ('a', 'DT'), ('basic', 'JJ'), ('knowledge', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Python', 'NNP'), ('programming', 'NN'), ('language', 'NN'), ('.', '.'), ('He/she', 'NNP'), ('must', 'MD'), ('have', 'VB'), ('knowledge', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('types', 'NNS'), ('in', 'IN'), ('Python.He', 'NNP'), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('write', 'VB'), ('functions', 'NNS'), (',', ','), ('and', 'CC'), ('also', 'RB'), ('have', 'VBP'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('import', 'NN'), ('and', 'CC'), ('use', 'NN'), ('libraries', 'NNS'), ('and', 'CC'), ('packages', 'NNS'), ('in', 'IN'), ('Python', 'NNP'), ('.', '.'), ('Familiarity', 'NN'), ('with', 'IN'), ('basic', 'JJ'), ('linguistics', 'NNS'), ('and', 'CC'), ('probability', 'NN'), ('is', 'VBZ'), ('assumed', 'VBN'), ('although', 'IN'), ('not', 'RB'), ('required', 'VBN'), ('to', 'TO'), ('fully', 'RB'), ('complete', 'VB'), ('this', 'DT'), ('course', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "print(pos_tag(corrected_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'reader', 'course', 'basic', 'knowledge', 'Python', 'programming', 'language', '.', 'He/she', 'must', 'knowledge', 'data', 'types', 'Python.He', 'able', 'write', 'functions', ',', 'also']\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stop_words(word_list):\n",
    "    corrected_word_list_without_stopwords = []\n",
    "    for wd in word_list:\n",
    "        if wd not in stop_words:\n",
    "            corrected_word_list_without_stopwords.append(wd)\n",
    "    return corrected_word_list_without_stopwords\n",
    "\n",
    "corrected_word_list_without_stopwords = remove_stop_words(corrected_word_list)\n",
    "print(corrected_word_list_without_stopwords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'reader', 'cours', 'basic', 'knowledg', 'python', 'program', 'languag', '.', 'he/sh', 'must', 'knowledg', 'data', 'type', 'python.h', 'abl', 'write', 'function', ',', 'also']\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "\n",
    "stemmer = stem.PorterStemmer()\n",
    "def get_stems(word_list):\n",
    "    corrected_word_list_without_stopwords_stemmed = []\n",
    "    for wd in word_list:\n",
    "        corrected_word_list_without_stopwords_stemmed.append(stemmer.stem(wd))\n",
    "    return corrected_word_list_without_stopwords_stemmed\n",
    "\n",
    "corrected_word_list_without_stopwords_stemmed = get_stems(corrected_word_list_without_stopwords)\n",
    "print(corrected_word_list_without_stopwords_stemmed[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'reader', 'cours', 'basic', 'knowledg', 'python', 'program', 'languag', '.', 'he/sh', 'must', 'knowledg', 'data', 'type', 'python.h', 'abl', 'write', 'function', ',', 'also']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_lemma(word_list):\n",
    "    corrected_word_list_without_stopwords_lemmatized = []\n",
    "    for wd in word_list:\n",
    "        corrected_word_list_without_stopwords_lemmatized.append(lemmatizer.lemmatize(wd))\n",
    "    return corrected_word_list_without_stopwords_lemmatized\n",
    "corrected_word_list_without_stopwords_lemmatized =  get_lemma(corrected_word_list_without_stopwords_stemmed)\n",
    "print(corrected_word_list_without_stopwords_lemmatized[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The reader of this course should have a basic knowledge of the Python programming language.', 'He/she must have knowledge of data types in Python.He should be able to write functions, and also have the ability to import and use libraries and packages in Python.', 'Familiarity with basic linguistics and probability is assumed although not required to fully complete this course.']\n"
     ]
    }
   ],
   "source": [
    "# 8\n",
    "\n",
    "print(sent_tokenize(corrected_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kick starting an nlp project\n",
    "\n",
    "## data collection\n",
    "\n",
    "This is the initial phase of any NLP project. Our sole purpose is to collect data as per our requirements. For this, we may either use existing data, collect data from various online repositories, or create our own dataset by crawling the web. In our case, we will collect different email data. We can even get this data from our personal emails as well, to start with.\n",
    "\n",
    "## data preprocessing\n",
    "\n",
    "Once the data is collected, we need to clean it using different preprocessing steps. It is necessary to clean the collected data to ensure effectiveness and accuracy:\n",
    "1. Converting all the text data to lowercase\n",
    "2. Stop word removal\n",
    "3. Text normalization, which will include replacing all numbers with some common term and replacing punctuation with empty strings\n",
    "4. Stemming and lemmatization\n",
    "\n",
    "## feature extraction\n",
    "\n",
    "Machine learning models tend to understand only numeric data. Therefore, it becomes necessary to convert text data into its equivalent numerical form.\n",
    "\n",
    "To convert every email into its equivalent numerical form, we will create a dictionary of all the unique words in our data and assign a unique index to each word. Then, we will represent every email with a list having a length equal to the number of unique words in the data. The list will have 1 at the indices of words that are present in the email and 0 at the other indices. This is called one-hot encoding. \n",
    "\n",
    "## model development\n",
    "\n",
    "Once the feature set is ready, we need to develop a suitable model that can be trained to gain knowledge from the data. These models are generally statistical, machine learning-based, deep learning-based, or reinforcement learning-based. In our case, we will build a model that is capable of differentiating between important and unimportant emails.\n",
    "\n",
    "## model assessment\n",
    "\n",
    "After developing a model, it is essential to benchmark it. This process of benchmarking is known as model assessment. In this step, we will evaluate the performance of our model by comparing it to others. This can be done by using different parameters or metrics. These parameters include precision, recall, and accuracy. In our case, we will evaluate the newly created model by seeing how well it performs at classifying emails as important and unimportant.\n",
    "\n",
    "## model deployment\n",
    "\n",
    "This is the final stage for most industrial NLP projects. In this stage, the models are put into production. They are either integrated into an existing system or new products are created by keeping this model as a base. In our case, we will deploy our model to production, so that it can classify emails as important and unimportant in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end notes\n",
    "\n",
    "Use Google News API to collect news."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
