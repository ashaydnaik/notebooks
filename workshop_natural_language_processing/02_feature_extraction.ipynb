{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction\n",
    "\n",
    "Here, we learn how to make text understandable to machine learning algorithms. \n",
    "\n",
    "To use a machine learning algorithm on textual data, we need a numerical or vector representation of text data. Before converting the text data into numerical form, we will need to pass it through some preprocessing steps such as tokenization, stemming, lemmatization, and stop-word removal. \n",
    "\n",
    "We will learn these preprocessing steps and how to extract features from the preprocessed text and convert them into vectors. We will explore two popular methods for feature extraction: **Bag of Words** and **Term Frequency-Inverse Document Frequency**, and various\n",
    "methods for finding similarity between different texts, how text data can be visualized.\n",
    "\n",
    "# types of data\n",
    "\n",
    "Two main ways to categorize data: by structure and by content.\n",
    "\n",
    "## structure\n",
    "\n",
    "In terms of structure, data can be divided into structured, semi-structured, unstructured.\n",
    "* Structured data is represented using a tabular format in a csv or excel file.\n",
    "* Semi-structured data is transformable into a tabular format from markup language (html, xml) files.\n",
    "* Unstructured data includes text corpora and images. We cannot extract information by simple parsing. We need an algorithm that understands the semantics of language.\n",
    "\n",
    "## content\n",
    "\n",
    "In terms of content, data can be text, image, audio, video.\n",
    "\n",
    "# cleaning text data\n",
    "\n",
    "Data cleaning is the art of extracting meaningful portions from data by eliminating unnecessary details. Libraries:\n",
    "* re: standard Python regex library\n",
    "* textblob: built on top of nltk\n",
    "* keras: neural network library built on TensorFlow\n",
    "\n",
    "## tokenization\n",
    "\n",
    "Tokenization is the process of splitting sentences into their constituents; that is, words and punctuation. \n",
    "\n",
    "### text cleaning and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a method that will delete all characters other than digits, alphabetical characters, and whitespaces from the text and split the text into tokens. Replace all non-alphanumeric characters with an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    return re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', 'New', 'Delhi', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', 'Awesome', 'airshow', 'india', 'official', 'indian', 'army', 'India', '70thRepublic', 'Day', 'For', 'more', 'photos', 'ping', 'me', 'sunil', 'photoking', 'com']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Sunil tweeted, \"Witnessing 70th Republic Day of India from Rajpath, \\\n",
    "New Delhi. Mesmerizing performance by Indian Army! Awesome airshow! @india_official \\\n",
    "@indian_army #India #70thRepublic_Day. For more photos ping me sunil@photoking.com :)\"'\n",
    "print(clean_text(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting n-grams\n",
    "\n",
    "Extract n-grams using three different methods: \n",
    "* custom-defined functions\n",
    "* nltk\n",
    "* textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'cute'], ['cute', 'little'], ['little', 'boy'], ['boy', 'is'], ['is', 'playing'], ['playing', 'with'], ['with', 'the'], ['the', 'kitten']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# custom-defined function\n",
    "\n",
    "def n_gram_extractor(sentence, n):\n",
    "    tokenList = []\n",
    "    tokens = re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        tokenList.append(tokens[i:i+n])\n",
    "    return tokenList\n",
    "\n",
    "print(n_gram_extractor('The cute little boy is playing with the kitten.', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'cute', 'little'], ['cute', 'little', 'boy'], ['little', 'boy', 'is'], ['boy', 'is', 'playing'], ['is', 'playing', 'with'], ['playing', 'with', 'the'], ['with', 'the', 'kitten']]\n"
     ]
    }
   ],
   "source": [
    "print(n_gram_extractor('The cute little boy is playing with the kitten.', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'cute'), ('cute', 'little'), ('little', 'boy'), ('boy', 'is'), ('is', 'playing'), ('playing', 'with'), ('with', 'the'), ('the', 'kitten.')]\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "\n",
    "from nltk import ngrams\n",
    "print(list(ngrams('The cute little boy is playing with the kitten.'.split(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'cute', 'little'), ('cute', 'little', 'boy'), ('little', 'boy', 'is'), ('boy', 'is', 'playing'), ('is', 'playing', 'with'), ('playing', 'with', 'the'), ('with', 'the', 'kitten.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams('The cute little boy is playing with the kitten.'.split(), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordList(['The', 'cute']), WordList(['cute', 'little']), WordList(['little', 'boy']), WordList(['boy', 'is']), WordList(['is', 'playing']), WordList(['playing', 'with']), WordList(['with', 'the']), WordList(['the', 'kitten'])]\n"
     ]
    }
   ],
   "source": [
    "# textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"The cute little boy is playing with the kitten.\")\n",
    "print(blob.ngrams(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing text with keras and textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from textblob import TextBlob\n",
    "\n",
    "sentence = 'Sunil tweeted, \"Witnessing 70th Republic Day of India from Rajpath, \\\n",
    "New Delhi. Mesmerizing performancesby Indian Army! Awesome airshow! @india_official \\\n",
    "@indian_army #India #70thRepublic_Day. For more photos ping me sunil@photoking.com :)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sunil', 'tweeted', 'witnessing', '70th', 'republic', 'day', 'of', 'india', 'from', 'rajpath', 'new', 'delhi', 'mesmerizing', 'performancesby', 'indian', 'army', 'awesome', 'airshow', 'india', 'official', 'indian', 'army', 'india', '70threpublic', 'day', 'for', 'more', 'photos', 'ping', 'me', 'sunil', 'photoking', 'com']\n"
     ]
    }
   ],
   "source": [
    "def get_keras_tokens(text):\n",
    "    return text_to_word_sequence(text)\n",
    "\n",
    "print(get_keras_tokens(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', 'New', 'Delhi', 'Mesmerizing', 'performancesby', 'Indian', 'Army', 'Awesome', 'airshow', 'india_official', 'indian_army', 'India', '70thRepublic_Day', 'For', 'more', 'photos', 'ping', 'me', 'sunil', 'photoking.com']\n"
     ]
    }
   ],
   "source": [
    "def get_textblob_tokens(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.words\n",
    "\n",
    "print(get_textblob_tokens(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## types of tokenizers\n",
    "\n",
    "nltk provides the following tokenizers:\n",
    "* Whitespace tokenizer: It splits a string wherever a space, tab, or newline character is present.\n",
    "* Tweet tokenizer: This is specifically designed for tokenizing tweets. It takes care of all the special characters and emojis used in tweets and returns clean tokens.\n",
    "* MWE tokenizer: MWE stands for Multi-Word Expression. Here, certain groups of multiple words are treated as one entity during tokenization, such as \"United States of America,\" \"People's Republic of China,\" \"not only,\" and \"but also.\" These predefined groups are added at the beginning with **mwe** methods.\n",
    "* Regular expression tokenizer: These tokenizers are developed using regular expressions. Sentences are split based on the occurrence of a specific pattern (a regular expression).\n",
    "* WordPunctTokenizer: This splits a piece of text into a list of alphabetical and non-alphabetical characters. It actually splits text into tokens using a fixed regex, that is, ``'\\w+|[^\\w\\s]+'``.\n",
    "\n",
    "### tokenizing text using various tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "sentence = 'Sunil tweeted, \"Witnessing 70th Republic Day of India from Rajpath, \\\n",
    "New Delhi. Mesmerizing performance by Indian Army! Awesome airshow! @india_official \\\n",
    "@indian_army #India #70thRepublic_Day. For more photos ping me sunil@photoking.com :)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', ',', '\"', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', ',', 'New', 'Delhi', '.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', '!', 'Awesome', 'airshow', '!', '@india_official', '@indian_army', '#India', '#70thRepublic_Day', '.', 'For', 'more', 'photos', 'ping', 'me', 'sunil@photoking.com', ':)', '\"']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_with_tweet_tokenizer(text):\n",
    "    tweet_tokenizer = TweetTokenizer() \n",
    "    return tweet_tokenizer.tokenize(text)\n",
    "    \n",
    "print(tokenize_with_tweet_tokenizer(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hashtags, emojis, websites, and Twitter IDs are extracted as single tokens. If we had used the white space tokenizer, we would have got hash, dots, and the @ symbol as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted,', '\"Witnessing', '70th', 'Republic_Day', 'of', 'India', 'from', 'Rajpath,', 'New', 'Delhi.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army!', 'Awesome', 'airshow!', '@india_official', '@indian_army', '#India', '#70thRepublic_Day.', 'For', 'more', 'photos', 'ping', 'me', 'sunil@photoking.com', ':)\"']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_with_mwe(text):\n",
    "    mwe_tokenizer = MWETokenizer([('Republic', 'Day')])\n",
    "    mwe_tokenizer.add_mwe(('Indian', 'Army'))\n",
    "    return mwe_tokenizer.tokenize(text.split())\n",
    "\n",
    "print(tokenize_with_mwe(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words \"Indian\" and \"Army!\", which should have been treated as a single identity, were treated separately. This is because \"Army!\" (not \"Army\") is treated as a token. Let's see how this can be fixed in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted,', '\"Witnessing', '70th', 'Republic_Day', 'of', 'India', 'from', 'Rajpath,', 'New', 'Delhi.', 'Mesmerizing', 'performance', 'by', 'Indian_Army', 'Awesome', 'airshow', '@india_official', '@indian_army', '#India', '#70thRepublic_Day.', 'For', 'more', 'photos', 'ping', 'me', 'sunil@photoking.com', ':)\"']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_with_mwe(sentence.replace('!','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', ',', '\"Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', ',', 'New', 'Delhi', '.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', '!', 'Awesome', 'airshow', '!', '@india_official', '@indian_army', '#India', '#70thRepublic_Day.', 'For', 'more', 'photos', 'ping', 'me', 'sunil', '@photoking.com', ':)\"']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_with_regex_tokenizer(text):\n",
    "    reg_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    return reg_tokenizer.tokenize(text)\n",
    "\n",
    "print(tokenize_with_regex_tokenizer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted,', '\"Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath,', 'New', 'Delhi.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army!', 'Awesome', 'airshow!', '@india_official', '@indian_army', '#India', '#70thRepublic_Day.', 'For', 'more', 'photos', 'ping', 'me', 'sunil@photoking.com', ':)\"']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_with_wst(text):\n",
    "    wh_tokenizer = WhitespaceTokenizer()\n",
    "    return wh_tokenizer.tokenize(text)\n",
    "\n",
    "print(tokenize_with_wst(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', ',', '\"', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', ',', 'New', 'Delhi', '.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', '!', 'Awesome', 'airshow', '!', '@', 'india_official', '@', 'indian_army', '#', 'India', '#', '70thRepublic_Day', '.', 'For', 'more', 'photos', 'ping', 'me', 'sunil', '@', 'photoking', '.', 'com', ':)\"']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_with_wordpunct_tokenizer(text):\n",
    "    wp_tokenizer = WordPunctTokenizer()\n",
    "    return wp_tokenizer.tokenize(text)\n",
    "\n",
    "print(tokenize_with_wordpunct_tokenizer(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming\n",
    "\n",
    "The process of converting a word back into its base form is known as stemming. It is essential to do this, because without it, algorithms would treat two or more different forms of the same word as different entities, despite them having the same semantic meaning.\n",
    "\n",
    "### RegexpStemmer\n",
    "\n",
    "RegexpStemmer uses regular expressions to check whether morphological or structural prefixes or suffixes are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love play football'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "def get_stems(text):\n",
    "    regex_stemmer = RegexpStemmer('ing$', min=4) \n",
    "    return ' '.join([regex_stemmer.stem(wd) for wd in text.split()])\n",
    "\n",
    "sentence = \"I love playing football\"\n",
    "get_stems(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PorterStemmer\n",
    "\n",
    "The Porter stemmer removes various morphological and inflectional endings (such as suffixes, prefixes, and the plural \"s\") from English words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'befor eat it would be nice to sanit your hand with a sanit'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "sentence = \"Before eating it would be nice to sanitize your hands with a sanitizer\"\n",
    "\n",
    "def get_stems(text):\n",
    "    ps_stemmer = PorterStemmer()\n",
    "    return ' '.join([ps_stemmer.stem(wd) for wd in text.split()])\n",
    " \n",
    "get_stems(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though stemming is a useful technique in NLP, it has a severe drawback. While \"eating\" has been converted into \"eat\", \"sanitize\" has also been converted into \"sanit\".\n",
    "\n",
    "## lemmatization\n",
    "\n",
    "Stemming often generates meaningless words. Lemmatization deals with such cases by using vocabulary and analyzing the words' morphologies. It returns the base forms of words that can be found in dictionaries. \n",
    "\n",
    "### performing lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sentence = '''The products produced by the process\n",
    "              today are far better than what it produces generally.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The product produced by the process today are far better than what it produce generally .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_lemmas(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) \\\n",
    "                     for word in word_tokenize(text)])\n",
    "\n",
    "get_lemmas(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### singularizing and pluralizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seashell seashores\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "sentence = TextBlob('She sells seashells on the seashore')\n",
    "\n",
    "def singularize(word):\n",
    "    return word.singularize()\n",
    "\n",
    "def pluralize(word):\n",
    "    return word.pluralize()\n",
    "\n",
    "sing = singularize(sentence.words[2])\n",
    "\n",
    "plur = pluralize(sentence.words[5])\n",
    "\n",
    "print(sing, plur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language translation\n",
    "\n",
    "The **translate** method of TextBlob can translate text from one language to another.\n",
    "\n",
    "### language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"very good\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def translate(text,from_l,to_l):\n",
    "    en_blob = TextBlob(text)\n",
    "    return en_blob.translate(from_lang=from_l, to=to_l)\n",
    "\n",
    "translate(text='muy bien',from_l='es',to_l='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop word removal\n",
    "\n",
    "nltk and textblob can be used to remove stop words from text. \n",
    "\n",
    "### removing stop words from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sells seashells seashore'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sentence = \"She sells seashells on the seashore\"\n",
    "\n",
    "def remove_stop_words(text,stop_word_list):\n",
    "    return ' '.join([word for word in word_tokenize(text) \n",
    "                     if word.lower() not in stop_word_list])\n",
    "\n",
    "custom_stop_word_list = ['she', 'on', 'the', 'am', 'is', 'not']\n",
    "remove_stop_words(sentence,custom_stop_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activity: extracting top keywords from the news article\n",
    "\n",
    "1. Open a Jupyter Notebook.\n",
    "2. Import nltk and any other necessary libraries.\n",
    "3. Define some functions to:\n",
    "    * load the text file, \n",
    "    * convert the string into lowercase, \n",
    "    * tokenize the text, \n",
    "    * remove the stop words, \n",
    "    * perform stemming on all the remaining tokens\n",
    "    * calculate the frequency of all these words.\n",
    "4. Load news_article.txt using a Python file reader into a single string.\n",
    "5. Convert the text string into lowercase.\n",
    "6. Split the string into tokens using a white space tokenizer.\n",
    "7. Remove any stop words.\n",
    "8. Perform stemming on all the tokens.\n",
    "9. Calculate the frequency of all the words after stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import download, stem\n",
    "\n",
    "# The below statement will download the stop word list\n",
    "# 'nltk_data/corpora/stopwords/' at home directory of your computer\n",
    "download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    news = ''.join([line for line in open(file_path,encoding='utf-8')])\n",
    "    return news\n",
    "\n",
    "# This method will take string as input and return the string\n",
    "#  converted into lowercase\n",
    "def to_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "# This will take a text string as input and return the token.\n",
    "wht = WhitespaceTokenizer()\n",
    "def tokenize_text(text):\n",
    "    return wht.tokenize(text=text)\n",
    "\n",
    "# This will remove stop word tokens from the token list.\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stop_words(token_list):\n",
    "    return [word for word in token_list if word not in stop_words]\n",
    "\n",
    "# This will take a token list as input and return stemmed token list\n",
    "stemmer = stem.PorterStemmer()\n",
    "def get_stems(token_list):\n",
    "    return [stemmer.stem(word) for word in token_list]\n",
    "\n",
    "# This method will generate a dict of word frequencies from list.\n",
    "def get_freq(stems):\n",
    "    freq_dict = {}\n",
    "    for t in stems:\n",
    "        freq_dict[t.strip()] = freq_dict.get(t.strip(), 0) + 1\n",
    "    return freq_dict\n",
    "\n",
    "# This method will sort the dictionary on the values and return the top n \n",
    "# keys of the dictionary.\n",
    "def get_top_n_words(freq_dict, n):\n",
    "    sorted_dict = sorted(freq_dict.items(), key=operator.itemgetter(1), \n",
    "                         reverse=True)\n",
    "    return [x[0] for x in sorted_dict][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['law', 'justic', 'european', 'parti', 'took', 'poland’']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4\n",
    "path = \"data/news_article.txt\"\n",
    "news_article = load_file(path)\n",
    "\n",
    "# 5\n",
    "lower_case_news_art = to_lower_case(text=news_article)\n",
    "\n",
    "# 6\n",
    "tokens = tokenize_text(lower_case_news_art)\n",
    "\n",
    "# 7\n",
    "removed_tokens = remove_stop_words(tokens)\n",
    "\n",
    "# 8\n",
    "stems = get_stems(removed_tokens)\n",
    "\n",
    "# 9\n",
    "freq_dict = get_freq(stems)\n",
    "\n",
    "top_keywords = get_top_n_words(freq_dict, 6)\n",
    "top_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction from texts\n",
    "\n",
    "To convert each textual sentence into a vector, we need to represent it as a set of features. This set of features should uniquely represent the text, though, individually, some of the features may be common across many textual sentences. Features can be classified into two different categories:\n",
    "1. General features: These features are statistical calculations and do not depend on the content of the text. Some examples of general features could be the number of tokens in the text, the number of characters in the text, and so on.\n",
    "2. Specific features: These features are dependent on the inherent meaning of the text and represent the semantics of the text. For example, the frequency of unique words in the text is a specific feature.\n",
    "\n",
    "## extracting general features from raw text\n",
    "\n",
    "### extracting general features from raw text\n",
    "\n",
    "These general features include:\n",
    "* detecting the number of words, \n",
    "* the presence of \"wh\" words \n",
    "* the language in which the text is written. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The interim budget for 2019 will be announced ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you know how much expectation the middle-cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>February is the shortest month in a year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This financial year will end on 31st March.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The interim budget for 2019 will be announced ...\n",
       "1  Do you know how much expectation the middle-cl...\n",
       "2          February is the shortest month in a year.\n",
       "3        This financial year will end on 31st March."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    ['The interim budget for 2019 will be announced on 1st February.'], \n",
    "    [ '''Do you know how much expectation the middle-class working \n",
    "         population is having from this budget?'''], \n",
    "    ['February is the shortest month in a year.'], \n",
    "    ['This financial year will end on 31st March.']])\n",
    "df.columns = ['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11\n",
       "1    15\n",
       "2     8\n",
       "3     8\n",
       "Name: number_of_words, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_num_words(df):\n",
    "    df['number_of_words'] = df['text'].apply(lambda x \\\n",
    "                                             : len(TextBlob(str(x)).words))\n",
    "    return df\n",
    "\n",
    "add_num_words(df)['number_of_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The is_present method will find the intersection between set of tokens of\n",
    "every sentence and the wh_words and will return true if the length of intersection set is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "Name: is_wh_words_present, dtype: bool"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_present(wh_words, df):\n",
    "    df['is_wh_words_present'] = df['text'].apply(lambda x : True if \\\n",
    "        len(set(TextBlob(str(x)).words).intersection(wh_words))>0 else False)\n",
    "    return df\n",
    "\n",
    "wh_words = set(['why', 'who', 'which', 'what', 'where', 'when', 'how'])\n",
    "\n",
    "is_present(wh_words, df)['is_wh_words_present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    en\n",
       "1    en\n",
       "2    en\n",
       "3    en\n",
       "Name: language, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_language(df):\n",
    "    df['language'] = df['text'].apply(lambda x : \\\n",
    "                                      TextBlob(str(x)).detect_language())\n",
    "    return df\n",
    "\n",
    "get_language(df)['language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting general features from text\n",
    "\n",
    "The dataset that we will be using here consists of random statements. Our objective is to find the frequency of various general features such as punctuation, uppercase and lowercase words, letters, digits, words, and whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "nltk.download('tagsets')\n",
    "from nltk.data import load\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LS', 'TO', 'VBN', \"''\", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS']\n"
     ]
    }
   ],
   "source": [
    "# See what different kinds of PoS nltk provides\n",
    "\n",
    "def get_tagsets():\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    return list(tagdict.keys())\n",
    " \n",
    "tag_list = get_tagsets()\n",
    " \n",
    "print(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LS</th>\n",
       "      <th>TO</th>\n",
       "      <th>VBN</th>\n",
       "      <th>''</th>\n",
       "      <th>WP</th>\n",
       "      <th>UH</th>\n",
       "      <th>VBG</th>\n",
       "      <th>JJ</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>--</th>\n",
       "      <th>...</th>\n",
       "      <th>MD</th>\n",
       "      <th>VB</th>\n",
       "      <th>WRB</th>\n",
       "      <th>NNP</th>\n",
       "      <th>EX</th>\n",
       "      <th>NNS</th>\n",
       "      <th>SYM</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LS   TO  VBN   ''   WP   UH  VBG   JJ  VBZ   --  ...   MD   VB  WRB  NNP  \\\n",
       "0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    EX  NNS  SYM   CC   CD  POS  \n",
       "0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  1.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This method will count occurrence of pos tags in each sentence.\n",
    "def get_pos_occurrence_freq(data, tag_list):\n",
    "    # Get list of sentences in text_list\n",
    "    text_list = data.text\n",
    "    \n",
    "    # create empty dataframe\n",
    "    feature_df = pd.DataFrame(columns=tag_list)\n",
    "    for text_line in text_list:\n",
    "        \n",
    "        # get pos tags of each word.\n",
    "        pos_tags = [j for i, j in pos_tag(word_tokenize(text_line))]\n",
    "        \n",
    "        # create a dict of pos tags and their frequency in given sentence.\n",
    "        row = dict(Counter(pos_tags))\n",
    "        feature_df = feature_df.append(row, ignore_index=True)\n",
    "    feature_df.fillna(0, inplace=True)\n",
    "    return feature_df\n",
    "\n",
    "tag_list = get_tagsets()\n",
    "\n",
    "data = pd.read_csv('data/data.csv', header=0)\n",
    "feature_df = get_pos_occurrence_freq(data, tag_list)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the intersection of set of punctuations in text and punctuation set imported from string module of python and find the length of intersection set in each row and add it to column \"num_of_unique_punctuations\" of data frame.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: num_of_unique_punctuations, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_punctuation_count(feature_df, data):\n",
    "\n",
    "    feature_df['num_of_unique_punctuations'] = data['text']. \\\n",
    "        apply(lambda x: len(set(x).intersection(set(punctuation))))\n",
    "    return feature_df\n",
    " \n",
    "feature_df = add_punctuation_count(feature_df, data)\n",
    " \n",
    "feature_df['num_of_unique_punctuations'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize text in every row and create a set of only capital words, then find the length of this set and add it to the column \"number_of_capital_words\" of dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: number_of_capital_words, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_capitalized_word_count(feature_df, data):\n",
    "    feature_df['number_of_capital_words'] = data['text'].\\\n",
    "        apply(lambda x: len([word for word in word_tokenize(str(x)) if word[0].isupper()]))\n",
    "    return feature_df\n",
    " \n",
    "feature_df = get_capitalized_word_count(feature_df, data)\n",
    " \n",
    "feature_df['number_of_capital_words'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code line will tokenize text in every row and create a set of only small words, then find the length of this set and add it to the column \"number_of_small_words\" of dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    3\n",
       "2    7\n",
       "3    3\n",
       "4    2\n",
       "Name: number_of_small_words, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_small_word_count(feature_df, data):\n",
    "    feature_df['number_of_small_words'] = data['text'].\\\n",
    "        apply(lambda x: len([word for word in word_tokenize(str(x)) if word[0].islower()]))\n",
    "    return feature_df\n",
    "\n",
    "feature_df = get_small_word_count(feature_df, data)\n",
    "feature_df['number_of_small_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19\n",
       "1    18\n",
       "2    28\n",
       "3    14\n",
       "4    13\n",
       "Name: number_of_alphabets, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_number_of_alphabets(feature_df, data):\n",
    "    # The below code line will break the text line in a list of\n",
    "    # characters in each row and add the count of that list into\n",
    "    # the columns `number_of_alphabets`\n",
    " \n",
    "    feature_df['number_of_alphabets'] = data['text']. \\\n",
    "        apply(lambda x: len([ch for ch in str(x) if ch.isalpha()]))\n",
    "    return feature_df\n",
    "feature_df = get_number_of_alphabets(feature_df, data)\n",
    "feature_df['number_of_alphabets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: number_of_digits, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_number_of_digit_count(feature_df, data):\n",
    "    # The below code line will break the text line in a list of\n",
    "    # digits in each row and add the count of that list into\n",
    "    # the columns `number_of_digits`\n",
    " \n",
    "    feature_df['number_of_digits'] = data['text']. \\\n",
    "        apply(lambda x: len([ch for ch in str(x) if ch.isdigit()]))\n",
    "    return feature_df\n",
    "feature_df = get_number_of_digit_count(feature_df, data)\n",
    "feature_df['number_of_digits'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    4\n",
       "2    9\n",
       "3    5\n",
       "4    3\n",
       "Name: number_of_words, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_number_of_words(feature_df, data):\n",
    "    # The below code line will break the text line in a list of\n",
    "    # words in each row and add the count of that list into\n",
    "    # the columns `number_of_digits`\n",
    " \n",
    "    feature_df['number_of_words'] = data['text'].apply(lambda x\n",
    "                                                       : len(word_tokenize(str(x))))\n",
    " \n",
    "    return feature_df\n",
    "\n",
    "feature_df = get_number_of_words(feature_df, data)\n",
    "feature_df['number_of_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    3\n",
       "2    7\n",
       "3    3\n",
       "4    2\n",
       "Name: number_of_white_spaces, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_number_of_whitespaces(feature_df, data):\n",
    "    # The below code line will generate list of white spaces\n",
    "    # in each row and add the length of that list into\n",
    "    # the columns `number_of_white_spaces`\n",
    " \n",
    "    feature_df['number_of_white_spaces'] = data['text']. \\\n",
    "        apply(lambda x: len([ch for ch in str(x) if ch.isspace()]))\n",
    " \n",
    "    return feature_df\n",
    " \n",
    "feature_df = get_number_of_whitespaces(feature_df, data)\n",
    "feature_df['number_of_white_spaces'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LS</th>\n",
       "      <th>TO</th>\n",
       "      <th>VBN</th>\n",
       "      <th>''</th>\n",
       "      <th>WP</th>\n",
       "      <th>UH</th>\n",
       "      <th>VBG</th>\n",
       "      <th>JJ</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>--</th>\n",
       "      <th>...</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>POS</th>\n",
       "      <th>num_of_unique_punctuations</th>\n",
       "      <th>number_of_capital_words</th>\n",
       "      <th>number_of_small_words</th>\n",
       "      <th>number_of_alphabets</th>\n",
       "      <th>number_of_digits</th>\n",
       "      <th>number_of_words</th>\n",
       "      <th>number_of_white_spaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LS   TO  VBN   ''   WP   UH  VBG   JJ  VBZ   --  ...   CC   CD  POS  \\\n",
       "0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "   num_of_unique_punctuations  number_of_capital_words  number_of_small_words  \\\n",
       "0                           0                        1                      4   \n",
       "1                           0                        1                      3   \n",
       "2                           1                        1                      7   \n",
       "3                           1                        1                      3   \n",
       "4                           0                        1                      2   \n",
       "\n",
       "   number_of_alphabets  number_of_digits  number_of_words  \\\n",
       "0                   19                 0                5   \n",
       "1                   18                 0                4   \n",
       "2                   28                 0                9   \n",
       "3                   14                 0                5   \n",
       "4                   13                 0                3   \n",
       "\n",
       "   number_of_white_spaces  \n",
       "0                       4  \n",
       "1                       3  \n",
       "2                       7  \n",
       "3                       3  \n",
       "4                       2  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words (BoW)\n",
    "\n",
    "It is a method of extracting features from raw texts. In this technique, we convert each sentence into a vector. The length of this vector is equal to the number of unique words in all the documents. This is done in two steps:\n",
    "1. The vocabulary or dictionary of all the words is generated.\n",
    "2. The document is represented in terms of the presence or absence of all words.\n",
    "A vocabulary or dictionary is created from all the unique possible words available in the corpus (all documents) and every single word is assigned a unique index number. In the second step, every document is represented by a list whose length is equal to the number of words in the vocabulary. \n",
    "\n",
    "### creating a bag of words\n",
    "\n",
    "Create a BoW representation for all the terms in a document and ascertain the 10 most frequent terms. Use **CountVectorizer** module from sklearn, which performs the following tasks:\n",
    "1. Tokenizes the collection of documents, also called a corpus \n",
    "2. Builds the vocabulary of unique words\n",
    "3. Converts a document into vectors using the previously built vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>arts</th>\n",
       "      <th>at</th>\n",
       "      <th>becomes</th>\n",
       "      <th>between</th>\n",
       "      <th>both</th>\n",
       "      <th>brained</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>left</th>\n",
       "      <th>natural</th>\n",
       "      <th>of</th>\n",
       "      <th>overlap</th>\n",
       "      <th>part</th>\n",
       "      <th>processing</th>\n",
       "      <th>right</th>\n",
       "      <th>science</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  and  are  arts  at  becomes  between  both  brained  data  ...  \\\n",
       "0   1    1    0     1   0        0        1     0        0     1  ...   \n",
       "1   0    1    2     1   0        0        0     0        2     0  ...   \n",
       "2   0    1    0     1   1        1        0     1        0     0  ...   \n",
       "3   0    0    0     0   0        0        0     0        0     1  ...   \n",
       "\n",
       "   language  left  natural  of  overlap  part  processing  right  science  \\\n",
       "0         0     0        0   0        1     0           0      0        2   \n",
       "1         0     1        0   0        0     0           0      1        1   \n",
       "2         0     0        0   0        0     0           0      0        1   \n",
       "3         1     0        1   1        0     1           1      0        1   \n",
       "\n",
       "   time  \n",
       "0     0  \n",
       "1     0  \n",
       "2     1  \n",
       "3     0  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_text(corpus):\n",
    "    \"\"\"\n",
    "    Will return a dataframe in which every row will be\n",
    "    vector representation of a document in corpus\n",
    "    :param corpus: input text corpus\n",
    "    :return: dataframe of vectors\n",
    "    \"\"\"\n",
    "    bag_of_words_model = CountVectorizer()\n",
    " \n",
    "    # performs the above described three tasks on the given data corpus.\n",
    "    dense_vec_matrix = bag_of_words_model.fit_transform(corpus).todense()\n",
    "    bag_of_word_df = pd.DataFrame(dense_vec_matrix)\n",
    "    bag_of_word_df.columns = sorted(bag_of_words_model.vocabulary_)\n",
    "    return bag_of_word_df\n",
    "\n",
    "corpus = [\n",
    "        'Data Science is an overlap between Arts and Science',\n",
    "        'Generally, Arts graduates are right-brained and Science graduates are left-brained',\n",
    "        'Excelling in both Arts and Science at a time becomes difficult',\n",
    "        'Natural Language Processing is a part of Data Science']\n",
    "df = vectorize_text(corpus)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>arts</th>\n",
       "      <th>brained</th>\n",
       "      <th>data</th>\n",
       "      <th>graduates</th>\n",
       "      <th>is</th>\n",
       "      <th>right</th>\n",
       "      <th>science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  and  are  arts  brained  data  graduates  is  right  science\n",
       "0   1    1    0     1        0     1          0   1      0        2\n",
       "1   0    1    2     1        2     0          2   0      1        1\n",
       "2   0    1    0     1        0     0          0   0      0        1\n",
       "3   0    0    0     0        0     1          0   1      0        1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bow_top_n(corpus, n):\n",
    "    \"\"\"\n",
    "      Will return a dataframe in which every row \n",
    "      will be represented by presence or absence of top 10 most \n",
    "      frequently occurring words in data corpus\n",
    "      :param corpus: input text corpus\n",
    "      :return: dataframe of vectors\n",
    "      \"\"\"\n",
    "    bag_of_words_model_small = CountVectorizer(max_features=n)\n",
    "    bag_of_word_df_small = pd.DataFrame(bag_of_words_model_small.fit_transform(corpus).todense())\n",
    "    bag_of_word_df_small.columns = sorted(bag_of_words_model_small.vocabulary_)\n",
    "    return bag_of_word_df_small\n",
    "df_2 = bow_top_n(corpus, 10)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method only considers the presence and absence of words in a sentence or document—not the frequency of the words/tokens in a document. If we are going to use the semantics of any sentence, the frequency of the words plays an important role. To overcome this issue, there is another feature extraction model called TFIDF.\n",
    "\n",
    "## Zipf's law\n",
    "\n",
    "According to Zipf's law, the number of times a word occurs in a corpus is inversely proportional to its rank in the frequency table. In simple terms, if the words in a corpus are arranged in descending order of their frequency of occurrence, then the frequency of the word at the ith rank will be proportional to 1/i:\n",
    "\n",
    "Frequency of a word = 1 / rank of word in vocabulary\n",
    "\n",
    "This also means that the frequency of the most frequent word will be twice the frequency of the second most frequent word. \n",
    "\n",
    "### Zipf's law\n",
    "\n",
    "Plot both the expected and actual ranks and frequencies of tokens with the help of Zipf's law. Use the 20newsgroups dataset provided by the sklearn library, which is a collection of newsgroup documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashaynaik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pylab import *\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words = stop_words + list(string.printable)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_prepare_data(stop_words):\n",
    "    \"\"\"\n",
    "    This method will load 20newsgroups data and \n",
    "    and remove stop words from it using given stop word list.\n",
    "    :param stop_words: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    newsgroups_data_sample = fetch_20newsgroups(subset='train')\n",
    "    tokenized_corpus = [word.lower() for sentence in newsgroups_data_sample['data'] \\\n",
    "                        for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', sentence)) \\\n",
    "                        if word.lower() not in stop_words]\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(corpus, n):\n",
    "    token_count_di = Counter(corpus)\n",
    "    return token_count_di.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ax', 62412), ('edu', 21321), ('subject', 12265), ('com', 12134), ('lines', 11835), ('organization', 11233), ('one', 9017), ('would', 8910), ('writes', 7844), ('article', 7438), ('people', 5977), ('like', 5868), ('university', 5589), ('posting', 5507), ('know', 5134), ('get', 4998), ('host', 4996), ('nntp', 4814), ('max', 4776), ('think', 4583), ('also', 4308), ('use', 4187), ('time', 4102), ('new', 3986), ('good', 3759), ('ca', 3546), ('could', 3511), ('well', 3480), ('us', 3364), ('may', 3313), ('even', 3280), ('see', 3065), ('cs', 3041), ('two', 3015), ('way', 3002), ('god', 2998), ('first', 2976), ('many', 2945), ('make', 2894), ('much', 2879), ('system', 2817), ('distribution', 2767), ('right', 2742), ('world', 2724), ('say', 2706), ('want', 2522), ('go', 2474), ('anyone', 2468), ('10', 2466), ('reply', 2463)]\n"
     ]
    }
   ],
   "source": [
    "stop_word_list = get_stop_words()\n",
    "corpus = get_and_prepare_data(stop_word_list)\n",
    "print(get_frequency(corpus, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAS3UlEQVR4nO3dfYxV9Z3H8c+XGWUGERxkKqNYR+NTrIrW6arrbmrEdn1o1BoJ2kFNy2Zi2V2oMelqbJfQrpvdpE808WliXbMrUYMdV0OwrlBdY1rYHcRSBFEKgijI2FofgOFh5rt/3DNw5+HO3Idz7zm/e9+v5Gbu+Z1z7/meOeTDb37nydxdAIDwjEu6AABAcQhwAAgUAQ4AgSLAASBQBDgABKq+kiubOnWqt7a2VnKVABC8NWvWfOjuzUPbKxrgra2t6u7uruQqASB4ZrZtpHaGUAAgUAQ4AASKAAeAQBHgABAoAhwAAhVEgO/8dKe+/NiXteuzXUmXAgCpEUSA//CVH+rV7a/qB//zg6RLAYDUsEreTratrc0LOQ+88b5G9R7qHdbeUN+gfffui7M0AEgtM1vj7m1D21PdA98yf4u+ce43NKF+giRpQv0EtZ/Xrq0LtiZcGQAkL9UB3nJsiyaNn6Tevl411Deot69Xk8ZP0rSJ05IuDQASl+oAl6QP9nygOy66Q6vmrtIdF93BgUwAiKR6DBwAEOgYOAAgNwIcAAJFgANAoAhwAAgUAQ4AgSLAASBQBDgABIoAB4BAEeAAECgCHAACRYADQKAIcAAI1JgBbmaPmtluM1uf1TbFzF40s7ejn03lLJJHqgHAcPn0wB+TdNWQtrslrXT3MyStjKbLhkeqAcBwed1O1sxaJS1z93Oj6U2SLnf3nWbWIulldz9rrO/hkWoAULi4byd7grvvjN7vknTCKCvuMLNuM+vu6ekpaCU8Ug0Aciv5IKZnuvA5u/Hu3unube7e1tzcXNB380g1AMit2AD/IBo6UfRzd3wlDVkRj1QDgBHVF/m55yTdLulfo5/PxlbREF2zuw6/v//a+8u1GgAITj6nET4h6beSzjKzHWY2V5ng/oqZvS3pymgaAFBBY/bA3f2WHLNmxlwLAKAAXIkJAIEKJsC5GhMABgsmwOc/P1+vbHtFC55fkHQpAJAKeV2JGZdCr8SUuBoTAOK+ErNiRgrv0doBoFakPsDXdqzNOa/xvsYKVgIA6ZL6AL+g5QJNOGrCsHbuiQKg1qU+wCVp8vjJamoYfMtx7okCoNYFEeDv3/W+Lm+9/HCINzU0cTohgJpX7L1QKsoW2aDpj3o/0jNvPiNbZPKFlTuLBgDSJIge+NqOtTpl8imD2qZPmq7f3fG7hCoCgOQFEeCXPnqptn28bVDbjk926OJHLk6oIgBIXhABvmX+lhHbew/1ciohgJoVRIC3HNuiW8+/dVBbvdVzKiGAmhZEgEvSZwc+0xemfuHw9CE/xKmEAGpaMAHeNbtLZ049U2dOOVOS1FjfqHf+/E6yRQFAgoI4jVAaflOrfYf26fnNz6vxvkZuagWgJgXTA89110QOZAKoVcEE+NYFW2WyEef19/dXuBoASF4wAd5ybItcI/fCD/QfGHa1JgBUu2ACXJKuPv3qUeczlAKglgQV4Mvbl2v6pOk551fy6UIAkLSgAlySvnTil5IuAQBSIbgA75rdpXE2ctn7+/YzFg6gZgQX4JK0484daqhrGNY+tXEqdygEUDOCDPCWY1u0v2//sPYP932otoeHPbgZAKpSkAEuSVedftWI7Qf9IMMoAGpCSQFuZnea2Rtmtt7MnjCz4eMaZbK8fbmuOf2a3LUtMq37YF2lygGAiis6wM3sJEnzJbW5+7mS6iTdHFdh+RhfP37U+TMemqGnNzxdoWoAoLJKHUKpl9RoZvWSJkh6v/SS8tc1u0tHjzt61GVmLZ3FkAqAqlR0gLv7e5J+JGm7pJ2SPnb3/x66nJl1mFm3mXX39PQUX2kO+7+/P+c9UgbVwZAKgCpTyhBKk6TrJZ0q6URJx5jZnKHLuXunu7e5e1tzc3PxlY7ihrNvUJ3VjbncrKWzyrJ+AEhCKUMoV0ra6u497n5QUpekv4ynrMJ0ze7SdWddl/MCnwFv/fEt2SJjSAVAVSglwLdLusTMJpiZSZopaWM8ZRWua3aX+v6pT18/++t5Lc9wCoDQlTIGvlrS05Jek/T76Ls6Y6qraF2zu+QLXU3jm0ZdbsZDMxgXBxC0kh6p5u4LJS2MqZZY7T20N6/lZjw04/D7pbOW6qZzbipXSQAQq2CvxBxL7/d65Qtdrce15v2ZgVMObZFx/jiA1AvmocbFunDahdr+8Xb1e2GPXcs+Y2WcjdOLt76oK069Iu7yAKBoVsmHILS1tXl3d3fF1pftxB+fqF2f7cr5WLZCMNQCoJLMbI27D7tTX9UOoQz1/l3v64azb4jlu7KHWib+y0QOhAJIRM30wLPd+NSNenbTswUPq+TDZFpx2wqGWwDEJlcPvCYDfKiGf24Y8f7icTn/c+frhVtf0LSJ08q2DgDVK1eAV/1BzHz0fq/38PtyhPm63evU8uOWQW0cGAVQKgJ8iOwwL+dQS7/3a+Z/zBzWbjKd97nz6LEDGBNDKAUo91DLaBrrG7Xqb1fp/BPOT2T9AJLDEEoMsnvncZ6WmI99h/YNumpUko456hj9Zu5vCHWgRhHgRXr/ruHPrqh0D33PwT3DQn0AY+xA9SPAY5TdQ5cyvfSdn+1MpJZcY+wDCHggfAR4GY3USy/ngdFCjBTwHEAFwsJBzBRJ8iDpWDiXHUgOBzEDMHQIRkpPj32kc9kHcDAVSAY98ADd+NSNeubNZ5Iuo2CcCgkUh0vpa0jdojr1K9kee74YdwfGxhBKDelb2JdzXlqGZAa4fNThGYnb9wK50AOHpGRPeYwbvXpUG4ZQUJRKX3FaaYQ9QkCAI3ZpG46pBM64QRIIcCSmFoN+AD18xIEAR+qk+cKltKHnX9s4CwWpM9KFS9kI+CNGu3FZLpx3X/0IcKTWWAE/EkL/iJFuQZwvevxhIMBRVQoJfcI+t2J6/CPhrpflVVKAm9lxkh6RdK4kl/Qtd/9tDHUBZVdI2Ff76ZTlMtZtjfPBhVy5ldoDXyzpV+5+k5kdLWlCDDUBqTPSrYHHUk0XRyVp1tJZRX3OZFpx24qq7v0XfRaKmU2W9Lqk0zzPL+EsFKAwtXwKZholdWA49tMIzewCSZ2SNkiaIWmNpAXuvifXZwhwoPwI/fQp9a+BcgR4m6RVki5z99VmtljSJ+7+/SHLdUjqkKTPf/7zF23btq2o9QEoP8K/fJoamvSnf/xTUZ8tR4BPk7TK3Vuj6b+WdLe7X5vrM/TAgdrBWT65+cLCcjf2C3ncfZeZvWtmZ7n7JkkzlRlOAYCizuPPVo3/AYyvG6/l7ctj+75Sz0L5B0lLojNQtkj6ZuklAUBp/wGkNfwnHDUh1rNiSgpwd39d0rBuPQAkqdTe/1BxHRvYe3BvTBVlcCUmAIyha3ZX0iWMaFzSBQAAikOAA0CgCHAACBQBDgCBIsABIFAEOAAEigAHgEAR4AAQKAIcAAJFgANAoAhwAAgUAQ4AgSLAASBQBDgABIoAB4BAEeAAECgCHAACRYADQKAIcAAIFAEOAIEiwAEgUAQ4AASKAAeAQBHgABAoAhwAAkWAA0CgSg5wM6szs7VmtiyOggAA+YmjB75A0sYYvgcAUICSAtzMpku6VtIj8ZQDAMhXqT3wn0n6rqT+XAuYWYeZdZtZd09PT4mrAwAMKDrAzexrkna7+5rRlnP3Tndvc/e25ubmYlcHABiilB74ZZKuM7N3JD0p6QozezyWqgAAYyo6wN39Hnef7u6tkm6W9Gt3nxNbZQCAUXEeOAAEqj6OL3H3lyW9HMd3AQDyQw8cAAJFgANAoAhwAAgUAQ4AgSLAASBQBDgABIoAB4BAEeAAECgCHAACRYADQKAIcAAIFAEOAIEiwAEgUAQ4AASKAAeAQBHgABAoAhwAAkWAA0CgCHAACBQBDgCBIsABIFAEOAAEigAHgEAR4AAQKAIcAAJFgANAoAhwAAhU0QFuZieb2UtmtsHM3jCzBXEWBgAYXX0Jnz0k6S53f83MjpW0xsxedPcNMdUGABhF0T1wd9/p7q9F7z+VtFHSSXEVBgAYXSxj4GbWKulCSatHmNdhZt1m1t3T0xPH6gAAiiHAzWyipF9K+o67fzJ0vrt3unubu7c1NzeXujoAQKSkADezo5QJ7yXu3hVPSQCAfJRyFopJ+oWkje7+k/hKAgDko5Qe+GWSbpV0hZm9Hr2uiakuAMAYij6N0N1flWQx1gIAKABXYgJAoAhwAAgUAQ4AgSLAASBQBDgABIoAB4BAEeAAECgCHAACRYADQKDSH+Dz5kn19ZJZ5ue8eUlXBACpUMoTecpv3jzpwQePTPf1HZl+4IFkagKAlEh3D7yzs7B2AKgh6Q7wvr7C2gGghqQ7wOvqCmsHgBqS7gDv6CisHQBqSLoPYg4cqOzszAyb1NVlwpsDmACQ8gCXMmFNYAPAMOkeQgEA5BRGgC9ZIrW2SuPGZX4uWZJ0RQCQuPQPoSxZkhn33rs3M71t25GDmO3tydUFAAlLfw/83nuPhPeAvXsz7QBQw9If4Nu3F9YOADUi/QE+ZUph7QBQI9If4L29I7d/+mll6wCAlEl/gO/ZM3L7gQOcjQKgpqU/wEczZw4hDqBmpT/Ajz9+9Plz5mQe9pDr1dhIyAOoSiUFuJldZWabzGyzmd0dV1GDLF5c2ud7e8cOeV68ePGqxOvKK+PJxUjRAW5mdZLul3S1pHMk3WJm58RV2GFcrAOgWqxcGWuIl9ID/wtJm919i7sfkPSkpOvjKWuIiRPL8rUAUHErV8b2VaUE+EmS3s2a3hG1DWJmHWbWbWbdPT09xa3poYeK+xwAVLGyH8R09053b3P3tubm5uK+pL1devzxzBgSAEBSaQH+nqSTs6anR23l0d4u9fdL3/522VYBAGU3c2ZsX1VKgP+fpDPM7FQzO1rSzZKei6esUTzwgOQ++ivGXxAAxGbmTGnFiti+rujbybr7ITP7e0kvSKqT9Ki7vxFbZaWI8RcEAGlV0v3A3X25pOUx1QIAKED6r8QEAIyIAAeAQBHgABAoAhwAAmXuXrmVmfVI2lbkx6dK+jDGckLANtcGtrk2lLLNp7j7sCshKxrgpTCzbndvS7qOSmKbawPbXBvKsc0MoQBAoAhwAAhUSAHemXQBCWCbawPbXBti3+ZgxsABAIOF1AMHAGQhwAEgUEEEeEUenlxhZnaymb1kZhvM7A0zWxC1TzGzF83s7ehnU9RuZvbz6Hewzsy+mOwWFM/M6sxsrZkti6ZPNbPV0bY9Fd2eWGY2PpreHM1vTbTwIpnZcWb2tJm9aWYbzezSat/PZnZn9O96vZk9YWYN1bafzexRM9ttZuuz2grer2Z2e7T822Z2eyE1pD7AK/bw5Mo7JOkudz9H0iWS/i7arrslrXT3MyStjKalzPafEb06JD1Y+ZJjs0DSxqzpf5P0U3c/XdJHkuZG7XMlfRS1/zRaLkSLJf3K3c+WNEOZba/a/WxmJ0maL6nN3c9V5nbTN6v69vNjkq4a0lbQfjWzKZIWSrpYmecMLxwI/by4e6pfki6V9ELW9D2S7km6rjJs57OSviJpk6SWqK1F0qbo/cOSbsla/vByIb2UeXLTSklXSFomyZS5Oq1+6P5W5l7zl0bv66PlLOltKHB7J0vaOrTuat7POvK83CnRflsm6W+qcT9LapW0vtj9KukWSQ9ntQ9abqxX6nvgyvPhySGL/mS8UNJqSSe4+85o1i5JJ0Tvq+X38DNJ35XUH00fL+nP7n4oms7ersPbHM3/OFo+JKdK6pH079Gw0SNmdoyqeD+7+3uSfiRpu6Sdyuy3Naru/Tyg0P1a0v4OIcCrmplNlPRLSd9x90+y53nmv+SqOc/TzL4mabe7r0m6lgqql/RFSQ+6+4WS9ujIn9WSqnI/N0m6Xpn/vE6UdIyGDzVUvUrs1xACvLIPT64gMztKmfBe4u5dUfMHZtYSzW+RtDtqr4bfw2WSrjOzdyQ9qcwwymJJx5nZwNOhsrfr8DZH8ydL+mMlC47BDkk73H11NP20MoFezfv5Sklb3b3H3Q9K6lJm31fzfh5Q6H4taX+HEODJPDy5zMzMJP1C0kZ3/0nWrOckDRyJvl2ZsfGB9tuio9mXSPo460+1ILj7Pe4+3d1bldmPv3b3dkkvSbopWmzoNg/8Lm6Klg+qp+ruuyS9a2ZnRU0zJW1QFe9nZYZOLjGzCdG/84Ftrtr9nKXQ/fqCpK+aWVP0l8tXo7b8JH0QIM8DBddIekvSHyTdm3Q9MW3TXynz59U6Sa9Hr2uUGftbKeltSSskTYmWN2XOxvmDpN8rc4Q/8e0oYfsvl7Qsen+apP+VtFnSUknjo/aGaHpzNP+0pOsuclsvkNQd7ev/ktRU7ftZ0iJJb0paL+k/JY2vtv0s6QllxvgPKvOX1txi9qukb0XbvlnSNwupgUvpASBQIQyhAABGQIADQKAIcAAIFAEOAIEiwAEgUAQ4AASKAAeAQP0/MeKgtJbqbXgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_actual_and_expected_frequencies(corpus):\n",
    "    freq_dict = get_frequency(corpus, 1000)\n",
    "    actual_frequencies = []\n",
    "    expected_frequencies = []\n",
    "    for rank, tup in enumerate(freq_dict):\n",
    "        actual_frequencies.append(log(tup[1]))\n",
    "        rank = 1 if rank == 0 else rank\n",
    "        # expected frequency 1/rank as per zipf’s law\n",
    "        expected_frequencies.append(1 / rank)\n",
    "    return actual_frequencies, expected_frequencies\n",
    " \n",
    "def plot(actual_frequencies, expected_frequencies):\n",
    "    plt.plot(actual_frequencies, 'g*', expected_frequencies, 'ro')\n",
    "    plt.show()\n",
    " \n",
    " \n",
    "# We will plot the actual and expected frequencies\n",
    "actual_frequencies, expected_frequencies = get_actual_and_expected_frequencies(corpus)\n",
    "plot(actual_frequencies, expected_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both lines have almost the same slope. In other words, we can say that the lines (or graphs) depict the proportionality of two lists.\n",
    "\n",
    "## term frequency - inverse document frequency (TFIDF)\n",
    "\n",
    "TFIDF is a method of representing text data in a vector format. We'll represent each document as a list whose length is equal to the number of unique words/tokens in all documents (corpus), but the vector here not only represents the presence and absence of a word, but also the frequency of the word — both in the current document and the whole corpus.\n",
    "\n",
    "This technique is based on the idea that the rarely occurring words are better representatives of the document than frequently occurring words. Hence, this representation gives more weightage to the rarer or less frequent words than frequently occurring words. It does so with the following formula:\n",
    "\n",
    "TFIDF = term frequency x inverse document frequency\n",
    "\n",
    "Term frequency is the frequency of a word in the given document. <br>\n",
    "Inverse document frequency can be defined as log(D/df), where df is document frequency and D is the total number of documents in the background corpus.\n",
    "\n",
    "### TFIDF representation\n",
    "\n",
    "Represent the input texts with their TFIDF vectors. Use sklearn module TfidfVectorizer, which converts text into TFIDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40332811 0.25743911 0.         0.25743911 0.         0.\n",
      "  0.40332811 0.         0.         0.31798852 0.         0.\n",
      "  0.         0.         0.         0.31798852 0.         0.\n",
      "  0.         0.         0.40332811 0.         0.         0.\n",
      "  0.42094668 0.        ]\n",
      " [0.         0.159139   0.49864399 0.159139   0.         0.\n",
      "  0.         0.         0.49864399 0.         0.         0.\n",
      "  0.24932199 0.49864399 0.         0.         0.         0.24932199\n",
      "  0.         0.         0.         0.         0.         0.24932199\n",
      "  0.13010656 0.        ]\n",
      " [0.         0.22444946 0.         0.22444946 0.35164346 0.35164346\n",
      "  0.         0.35164346 0.         0.         0.35164346 0.35164346\n",
      "  0.         0.         0.35164346 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.18350214 0.35164346]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30887228 0.         0.\n",
      "  0.         0.         0.         0.30887228 0.39176533 0.\n",
      "  0.39176533 0.39176533 0.         0.39176533 0.39176533 0.\n",
      "  0.2044394  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_tf_idf_vectors(corpus):\n",
    "    tfidf_model = TfidfVectorizer()\n",
    "    vector_list = tfidf_model.fit_transform(corpus).todense()\n",
    "    return vector_list\n",
    "\n",
    "\n",
    "corpus = [\n",
    "        'Data Science is an overlap between Arts and Science',\n",
    "        'Generally, Arts graduates are right-brained and Science graduates are left-brained',\n",
    "        'Excelling in both Arts and Science at a time becomes difficult',\n",
    "        'Natural Language Processing is a part of Data Science'\n",
    "    ]\n",
    "\n",
    "vector_list = get_tf_idf_vectors(corpus)\n",
    "print(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
